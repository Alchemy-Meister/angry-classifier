\chapter{Conclusion and Future Work}

In conclusion, in thesis we have done introduced the topic of automatic detection of repressed anger from text messages. Firstly, we described the research methodology and the planning that has been carried out its completion. Later, we have made a review of the research topics which this investigation is based on: (a) sentiment analysis and (b) emotion detection, followed by an introduction of relevant papers and previous work done, concluding the state of the art with a formal explanation about the fundamentals of the techniques that were going to be used to develop our approach.

Thus, with this base, we explained the design of our system, reviewing work done throughout all the steps that compose the procedure developed. On the next chapter, we enumerated all the experiments done to try to improve the performance of our model and compared designed approach with a naive classifier. In the results, we could see an improvement in the performance by evaluation the naive classifier and the variations of our solution with a manual imbalanced dataset based on real-life sentences extracted from users of Twitter social media. From zero chances of detecting repressed anger our first approach obtained a 0.22 F1-Score, proving the hypothesis \textit{``Can the repressed anger be detected in text?''} to be true even though, the results obtained with this approach are low, reflecting how complex the task is.

As the tweets usually contain slang, acronyms, spelling mistakes and expressions from oral language processing all the information can become a real challenge that can threat the performance of the model in the learning phase. With this idea in mind, the motivation of the variations performed to the proposed solution aim to improve the tweet pre-processing allowing to out model to learn from a wider feature set. Therefore, an alternative Word2vec pre-trained model that was build from millions of tweets was proposed as an alternative to the wide used Google News pre-trained model. Even though that this variation sightly improved the overall F1-Score performance (less than a thousandth), the detection of repressed anger decreased from a 0.22 to 0.15, showing that even a pre-trained model of 400 features that learned from tweets, cannot compete with the 300 feature pre-trained model created by Google from thousands of millions of words from news.

By analyzing the previous results, the following experiment focuses on making the Google pre-trained model recognize more words from which new features could be learned. From previous researches that used Word2vec model state that when word does not appear into the model the following actions can be performed: (a) stemming, (b) lemmatization, (c) synonyms, (d) morphological analyzers, (e) spelling correction, (f) slang and acronyms processing and (g) retrain the Word2vec model. Since the last option was not a viable, the securest approach to implement was the spelling correction, since if the word to be processed is wrong spelled nor lemmatization, synonyms or other options would work. Thus, before other options would work the correction of the spelling should be ensured and therefore, this approach was implemented. Applying the spell checker to the Google News pre-trained model increased the overall performance of the model sightly more than with the Twitter pre-trained model (less than a thousandth). However, with this implementation the performance on repressed anger detection did not decreased, in fact increased from 0.22 to 0.23 F1-Score, becoming in the solution that sightly outperformed the rest in precision, recall and F1-Score.

Without changing the neural design of the model, the tests conducted show us how important having a proper trained Word2vec model is, affecting in how the predictor classified the tweet instances into each categories. The usage of a simple spell checker and corrector has proven that more work into pre-processing phase would lead this model to score better results. There is still lots of work to be done in this aspect. Currently the spell checker only corrects words one by one isolatedly archiving, as shown in table \ref{tab:language_model_evaluation}, a 75\% and 69\% of reliability on the evaluation tasks. Norvig suggests that a \textit{``spelling corrector that scores 90\% accuracy will need to use the context of the surrounding words to make a choice''}. We this variation of the spell checking implementation would be viable, as Google has released a English word frequency list that contains word sequences up to 5-grams gathered from corpus from trillions of words \cite{LDCGoogle5-gram}. Once the spell checker has obtained a relatively high average of words correction, it would be viable to implement the remaining presented approaches to improve the initialization of space models in the word embedding matrices.

Once the pre-processing of the corpus is fair enough, modifications of the neural design could also have a big impact of the model's performance. As for example, \cite{ioffe2015batch} and \cite{ba2016layer} show that by adding a ReLU activation function layer with batch normalization could both, considerably increase model's accuracy and reduce the learning process spent time. Another factor that should be work on is the model's hyperparmeter optimization. All the proposed variations have been executed with a fixed setting of 200 filters of 3-grams, 4-grams and 5-grams size. The modification of this values with multiple combinations could lead to positively improve the performance.

It is relevant to indicate that, although the binary anger and irony classified performed relative good, with a minimum F1-Score of 0.78 and 0.85 respectively, the repressed anger predictor did not achieve that level of reliability, obtaining a little more than 0.39 F1-Score in the best case. Taking advantage of the manual annotated dataset created to evaluate this model, an alternative approach to the proposed solution could be a similar but single neural network that learn directly from the manual annotated data instead from the automatically annotated binary anger and irony datasets. By using distant or semi-supervised learning techniques it should be possible to used the small amount of annotated data as base from which determine the relevant features that defines repressed anger and then, find similitudes with the remaining majority of unlabeled data from which new features could be exploited in the training phase. This approach would enable to learn the features altogether instead of separate them into anger and irony, which might lead to solve the deficiencies found on the merging process of the designed predictor.

Finally, a more in depth psychological definition on repressed anger would also promote the definition of to new subtasks in this domain, such as in profiling the personality of individual users by exploiting more complex attributes that characterize people suffering from repressing anger. The results of this subtask could be applied on domains in which the detection of this personality disorders can help analyze the well being or health of the subjects.  